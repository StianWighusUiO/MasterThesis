map_dfr(compute_rejection_curve)
ggplot(rejection_data, aes(x = coverage, y = acc, color = model)) +
geom_line(linewidth = 1) +
scale_color_brewer(palette = "Set1") +
labs(
title = "Accuracy vs. Coverage (Seen Only)",
x = "Coverage (Top Confidence)",
y = "Accuracy",
color = "Model"
) +
theme_minimal(base_size = 13)
ggsave("rejection_curve.pdf", width = 8, height = 5)
nb_preds
nb_preds[, , 1]
nb_preds
nb_preds[, 1]
attr(nb_preds, "prob")
nb_probs
View(nb_probs)
predict(nb_model, filter(test, label == 5), prob = TRUE)
x <- predict(nb_model, filter(test, label == 5), prob = TRUE)
attr(x)
attr(x, "prob")
X <- attr(x, "prob")
View(X)
library(readr)
library(dplyr)
library(purrr)
library(ggplot2)
library(glmnet)
library(caret)
library(bnlearn)
library(tidyr)
library(RColorBrewer)
library(cowplot)
library(scales)
time_start <- Sys.time()
train <- read_delim("zip.train", delim = " ", col_names = FALSE)
test <- read_delim("zip.test", delim = " ", col_names = FALSE)
new_colnames <- c("label", paste0("X", rep(1:16, each = 16), "x", rep(1:16, times = 16)))
n_levels <- 3
train <- train %>%
select(!X258) %>%
rename_with(~ new_colnames, everything()) %>%
mutate(across("label", as.factor)) %>%
mutate(across(-label, ~ cut(.x, breaks = seq(-1, 1, 2 / n_levels), ordered_result = TRUE, include.lowest = TRUE))) %>%
as.data.frame()
test <- test %>%
rename_with(~ new_colnames, everything()) %>%
mutate(across("label", as.factor)) %>%
mutate(across(-label, ~ cut(.x, breaks = seq(-1, 1, 2 / n_levels), ordered_result = TRUE, include.lowest = TRUE))) %>%
as.data.frame()
cat("Data loaded and preprocessed:\n")
print(Sys.time() - time_start)
excluded_digit <- "5"
train_ood <- train %>% filter(label != excluded_digit)
train_ood$label <- droplevels(train_ood$label)
compute_entropy <- function(probs) {
-sum(probs * log(probs + 1e-10))
}
results_list <- list()
# 1. Naive Bayes
nb_structure <- naive.bayes(train_ood, "label")
nb_model <- bn.fit(nb_structure, train_ood, method = "bayes")
nb_preds <- predict(nb_model, test, prob = TRUE)
nb_probs <- attr(nb_preds, "prob")
results_list[["nb"]] <- data.frame(
model = "nb",
true_label = test$label,
predicted_label = nb_preds,
max_prob = apply(nb_probs, 2, max),
entropy = apply(nb_probs, 2, compute_entropy),
is_ood = test$label == excluded_digit
)
cat("Naive Bayes done\n")
# 2. TAN
tan_structure <- tree.bayes(train_ood, "label")
tan_model <- bn.fit(tan_structure, train_ood, method = "bayes")
tan_preds <- predict(tan_model, test, prob = TRUE)
tan_probs <- attr(tan_preds, "prob")
results_list[["tan"]] <- data.frame(
model = "tan",
true_label = test$label,
predicted_label = tan_preds,
max_prob = apply(tan_probs, 2, max),
entropy = apply(tan_probs, 2, compute_entropy),
is_ood = test$label == excluded_digit
)
cat("TAN done\n")
# === Prepare logistic regression data ===
x_train <- model.matrix(label ~ ., train_ood)[, -1]
y_train <- as.numeric(as.character(train_ood$label))
x_test <- model.matrix(label ~ ., test)[, -1]
y_test <- as.numeric(as.character(test$label))
# 3. Logistic Regression
# logit_cv <- cv.glmnet(x_train, y_train, family = "multinomial", alpha = 1)
logit_probs <- predict(logit_cv$glmnet.fit, newx = x_test, s = logit_cv$lambda.1se, type = "response")[,,1]
logit_preds <- colnames(logit_probs)[apply(logit_probs, 1, which.max)]
results_list[["logreg"]] <- data.frame(
model = "logreg",
true_label = test$label,
predicted_label = logit_preds,
max_prob = apply(logit_probs, 1, max),
entropy = apply(logit_probs, 1, compute_entropy),
is_ood = test$label == excluded_digit
)
cat("Logistic regression done\n")
# 4. Logistic Regression with TAN interactions
interactions <- as.data.frame(tan_structure$arcs) %>%
filter(from != "label" & to != "label") %>%
transmute(term = paste0(from, ":", to)) %>%
pull(term)
formula_tan <- paste0("label ~ . + ", paste(interactions, collapse = " + "))
x_TAN_train <- model.matrix(as.formula(formula_tan), train_ood)[, -1]
y_TAN_train <- as.numeric(as.character(train_ood$label))
x_TAN_test <- model.matrix(as.formula(formula_tan), test)[, -1]
y_TAN_test <- as.numeric(as.character(test$label))
# logit_tan_cv <- cv.glmnet(x_TAN_train, y_TAN_train, family = "multinomial", alpha = 1)
logit_tan_probs <- predict(logit_tan_cv$glmnet.fit, newx = x_TAN_test, s = logit_tan_cv$lambda.1se, type = "response")[,,1]
logit_tan_preds <- colnames(logit_tan_probs)[apply(logit_tan_probs, 1, which.max)]
results_list[["logreg_tan"]] <- data.frame(
model = "logreg_tan",
true_label = test$label,
predicted_label = logit_tan_preds,
max_prob = apply(logit_tan_probs, 1, max),
entropy = apply(logit_tan_probs, 1, compute_entropy),
is_ood = test$label == excluded_digit
)
cat("Logistic regression + TAN done\n")
# === Combine and save results ===
df <- bind_rows(results_list)
write_csv(df, "ood_results.csv")
cat("Saved: ood_results.csv\n")
# === Plotting ===
df <- df %>%
mutate(
is_ood = factor(is_ood, levels = c(FALSE, TRUE), labels = c("Seen", "OOD")),
model = factor(model, levels = c("nb", "tan", "logreg", "logreg_tan"),
labels = c("Naive Bayes", "TAN", "LogReg", "LogReg+TAN"))
)
# --- Confidence Histogram ---
ggplot(df, aes(x = max_prob, fill = is_ood)) +
geom_histogram(binwidth = 0.05, alpha = 0.7, position = "identity") +
facet_wrap(~ model, ncol = 2) +
scale_fill_manual(values = c("Seen" = "#0072B2", "OOD" = "#D55E00")) +
labs(
title = "Prediction Confidence: Seen vs. OOD",
x = "Max Predicted Probability",
y = "Count",
fill = "Sample"
) +
theme_minimal(base_size = 13)
ggsave("confidence_histogram.pdf", width = 8, height = 5)
# --- Entropy Boxplot ---
ggplot(df, aes(x = model, y = entropy, fill = is_ood)) +
geom_boxplot(alpha = 0.8, outlier.size = 0.5) +
scale_fill_manual(values = c("Seen" = "#0072B2", "OOD" = "#D55E00")) +
labs(
title = "Entropy by Model and Sample Type",
x = "Model",
y = "Entropy",
fill = "Sample"
) +
theme_minimal(base_size = 13)
ggsave("entropy_boxplot.pdf", width = 8, height = 5)
# --- Rejection Curve ---
compute_rejection_curve <- function(df_model) {
df_model %>%
arrange(desc(max_prob)) %>%
mutate(
rank = row_number(),
coverage = rank / n(),
correct = predicted_label == true_label
) %>%
group_by(coverage = round(coverage, 2)) %>%
summarise(acc = mean(correct), .groups = "drop") %>%
mutate(model = unique(df_model$model))
}
rejection_data <- df %>%
filter(is_ood == "Seen") %>%
group_split(model) %>%
map_dfr(compute_rejection_curve)
ggplot(rejection_data, aes(x = coverage, y = acc, color = model)) +
geom_line(linewidth = 1) +
scale_color_brewer(palette = "Set1") +
labs(
title = "Accuracy vs. Coverage (Seen Only)",
x = "Coverage (Top Confidence)",
y = "Accuracy",
color = "Model"
) +
theme_minimal(base_size = 13)
ggsave("rejection_curve.pdf", width = 8, height = 5)
compute_entropy(nb_probs)
nb_probs
nb_probs[1, 1]
nb_probs[, 1]
compute_entropy(nb_probs[, 1])
results_list$nb
(results_list$nb)$entropy
results_list$logreg
library(readr)
library(dplyr)
library(purrr)
library(ggplot2)
library(glmnet)
library(caret)
library(bnlearn)
library(tidyr)
library(RColorBrewer)
library(cowplot)
library(scales)
time_start <- Sys.time()
train <- read_delim("zip.train", delim = " ", col_names = FALSE)
test <- read_delim("zip.test", delim = " ", col_names = FALSE)
new_colnames <- c("label", paste0("X", rep(1:16, each = 16), "x", rep(1:16, times = 16)))
n_levels <- 3
train <- train %>%
select(!X258) %>%
rename_with(~ new_colnames, everything()) %>%
mutate(across("label", as.factor)) %>%
mutate(across(-label, ~ cut(.x, breaks = seq(-1, 1, 2 / n_levels), ordered_result = TRUE, include.lowest = TRUE))) %>%
as.data.frame()
test <- test %>%
rename_with(~ new_colnames, everything()) %>%
mutate(across("label", as.factor)) %>%
mutate(across(-label, ~ cut(.x, breaks = seq(-1, 1, 2 / n_levels), ordered_result = TRUE, include.lowest = TRUE))) %>%
as.data.frame()
cat("Data loaded and preprocessed:\n")
print(Sys.time() - time_start)
excluded_digit <- "5"
train_ood <- train %>% filter(label != excluded_digit)
train_ood$label <- droplevels(train_ood$label)
compute_entropy <- function(probs) {
-sum(probs * log(probs + 1e-10))
}
results_list <- list()
# 1. Naive Bayes
nb_structure <- naive.bayes(train_ood, "label")
nb_model <- bn.fit(nb_structure, train_ood, method = "bayes")
nb_preds <- predict(nb_model, test, prob = TRUE)
nb_probs <- attr(nb_preds, "prob")
results_list[["nb"]] <- data.frame(
model = "nb",
true_label = test$label,
predicted_label = nb_preds,
max_prob = apply(nb_probs, 2, max),
entropy = apply(nb_probs, 2, compute_entropy),
is_ood = test$label == excluded_digit
)
cat("Naive Bayes done\n")
# 2. TAN
tan_structure <- tree.bayes(train_ood, "label")
tan_model <- bn.fit(tan_structure, train_ood, method = "bayes")
tan_preds <- predict(tan_model, test, prob = TRUE)
tan_probs <- attr(tan_preds, "prob")
results_list[["tan"]] <- data.frame(
model = "tan",
true_label = test$label,
predicted_label = tan_preds,
max_prob = apply(tan_probs, 2, max),
entropy = apply(tan_probs, 2, compute_entropy),
is_ood = test$label == excluded_digit
)
cat("TAN done\n")
# === Prepare logistic regression data ===
x_train <- model.matrix(label ~ ., train_ood)[, -1]
y_train <- as.numeric(as.character(train_ood$label))
x_test <- model.matrix(label ~ ., test)[, -1]
y_test <- as.numeric(as.character(test$label))
# 3. Logistic Regression
# logit_cv <- cv.glmnet(x_train, y_train, family = "multinomial", alpha = 1)
logit_probs <- predict(logit_cv$glmnet.fit, newx = x_test, s = logit_cv$lambda.1se, type = "class")[,,1]
library(readr)
library(dplyr)
library(purrr)
library(ggplot2)
library(glmnet)
library(caret)
library(bnlearn)
library(tidyr)
library(RColorBrewer)
library(cowplot)
library(scales)
time_start <- Sys.time()
train <- read_delim("zip.train", delim = " ", col_names = FALSE)
test <- read_delim("zip.test", delim = " ", col_names = FALSE)
new_colnames <- c("label", paste0("X", rep(1:16, each = 16), "x", rep(1:16, times = 16)))
n_levels <- 3
train <- train %>%
select(!X258) %>%
rename_with(~ new_colnames, everything()) %>%
mutate(across("label", as.factor)) %>%
mutate(across(-label, ~ cut(.x, breaks = seq(-1, 1, 2 / n_levels), ordered_result = TRUE, include.lowest = TRUE))) %>%
as.data.frame()
test <- test %>%
rename_with(~ new_colnames, everything()) %>%
mutate(across("label", as.factor)) %>%
mutate(across(-label, ~ cut(.x, breaks = seq(-1, 1, 2 / n_levels), ordered_result = TRUE, include.lowest = TRUE))) %>%
as.data.frame()
cat("Data loaded and preprocessed:\n")
print(Sys.time() - time_start)
excluded_digit <- "5"
train_ood <- train %>% filter(label != excluded_digit)
train_ood$label <- droplevels(train_ood$label)
compute_entropy <- function(probs) {
-sum(probs * log(probs + 1e-10))
}
results_list <- list()
# 1. Naive Bayes
nb_structure <- naive.bayes(train_ood, "label")
nb_model <- bn.fit(nb_structure, train_ood, method = "bayes")
nb_preds <- predict(nb_model, test, prob = TRUE)
nb_probs <- attr(nb_preds, "prob")
results_list[["nb"]] <- data.frame(
model = "nb",
true_label = test$label,
predicted_label = nb_preds,
max_prob = apply(nb_probs, 2, max),
entropy = apply(nb_probs, 2, compute_entropy),
is_ood = test$label == excluded_digit
)
cat("Naive Bayes done\n")
# 2. TAN
tan_structure <- tree.bayes(train_ood, "label")
tan_model <- bn.fit(tan_structure, train_ood, method = "bayes")
tan_preds <- predict(tan_model, test, prob = TRUE)
tan_probs <- attr(tan_preds, "prob")
results_list[["tan"]] <- data.frame(
model = "tan",
true_label = test$label,
predicted_label = tan_preds,
max_prob = apply(tan_probs, 2, max),
entropy = apply(tan_probs, 2, compute_entropy),
is_ood = test$label == excluded_digit
)
cat("TAN done\n")
# === Prepare logistic regression data ===
x_train <- model.matrix(label ~ ., train_ood)[, -1]
y_train <- as.numeric(as.character(train_ood$label))
x_test <- model.matrix(label ~ ., test)[, -1]
y_test <- as.numeric(as.character(test$label))
# 3. Logistic Regression
# logit_cv <- cv.glmnet(x_train, y_train, family = "multinomial", alpha = 1)
logit_probs <- predict(logit_cv$glmnet.fit, newx = x_test, s = logit_cv$lambda.1se, type = "response")[,,1]
logit_preds <- colnames(logit_probs)[apply(logit_probs, 1, which.max)]
results_list[["logreg"]] <- data.frame(
model = "logreg",
true_label = test$label,
predicted_label = logit_preds,
max_prob = apply(logit_probs, 1, max),
entropy = apply(logit_probs, 1, compute_entropy),
is_ood = test$label == excluded_digit
)
cat("Logistic regression done\n")
# 4. Logistic Regression with TAN interactions
interactions <- as.data.frame(tan_structure$arcs) %>%
filter(from != "label" & to != "label") %>%
transmute(term = paste0(from, ":", to)) %>%
pull(term)
formula_tan <- paste0("label ~ . + ", paste(interactions, collapse = " + "))
x_TAN_train <- model.matrix(as.formula(formula_tan), train_ood)[, -1]
y_TAN_train <- as.numeric(as.character(train_ood$label))
x_TAN_test <- model.matrix(as.formula(formula_tan), test)[, -1]
y_TAN_test <- as.numeric(as.character(test$label))
# logit_tan_cv <- cv.glmnet(x_TAN_train, y_TAN_train, family = "multinomial", alpha = 1)
logit_tan_probs <- predict(logit_tan_cv$glmnet.fit, newx = x_TAN_test, s = logit_tan_cv$lambda.1se, type = "response")[,,1]
logit_tan_preds <- colnames(logit_tan_probs)[apply(logit_tan_probs, 1, which.max)]
results_list[["logreg_tan"]] <- data.frame(
model = "logreg_tan",
true_label = test$label,
predicted_label = logit_tan_preds,
max_prob = apply(logit_tan_probs, 1, max),
entropy = apply(logit_tan_probs, 1, compute_entropy),
is_ood = test$label == excluded_digit
)
cat("Logistic regression + TAN done\n")
# === Combine and save results ===
df <- bind_rows(results_list)
write_csv(df, "ood_results.csv")
cat("Saved: ood_results.csv\n")
# === Plotting ===
df <- df %>%
mutate(
is_ood = factor(is_ood, levels = c(FALSE, TRUE), labels = c("Seen", "OOD")),
model = factor(model, levels = c("nb", "tan", "logreg", "logreg_tan"),
labels = c("Naive Bayes", "TAN", "LogReg", "LogReg+TAN"))
)
# --- Confidence Histogram ---
ggplot(df, aes(x = max_prob, fill = is_ood)) +
geom_histogram(binwidth = 0.05, alpha = 0.7, position = "identity") +
facet_wrap(~ model, ncol = 2) +
scale_fill_manual(values = c("Seen" = "#0072B2", "OOD" = "#D55E00")) +
labs(
title = "Prediction Confidence: Seen vs. OOD",
x = "Max Predicted Probability",
y = "Count",
fill = "Sample"
) +
theme_minimal(base_size = 13)
ggsave("confidence_histogram.pdf", width = 8, height = 5)
# --- Entropy Boxplot ---
ggplot(df, aes(x = model, y = entropy, fill = is_ood)) +
geom_boxplot(alpha = 0.8, outlier.size = 0.5) +
scale_fill_manual(values = c("Seen" = "#0072B2", "OOD" = "#D55E00")) +
labs(
title = "Entropy by Model and Sample Type",
x = "Model",
y = "Entropy",
fill = "Sample"
) +
theme_minimal(base_size = 13)
ggsave("entropy_boxplot.pdf", width = 8, height = 5)
# --- Rejection Curve ---
compute_rejection_curve <- function(df_model) {
df_model %>%
arrange(desc(max_prob)) %>%
mutate(
rank = row_number(),
coverage = rank / n(),
correct = predicted_label == true_label
) %>%
group_by(coverage = round(coverage, 2)) %>%
summarise(acc = mean(correct), .groups = "drop") %>%
mutate(model = unique(df_model$model))
}
rejection_data <- df %>%
filter(is_ood == "Seen") %>%
group_split(model) %>%
map_dfr(compute_rejection_curve)
ggplot(rejection_data, aes(x = coverage, y = acc, color = model)) +
geom_line(linewidth = 1) +
scale_color_brewer(palette = "Set1") +
labs(
title = "Accuracy vs. Coverage (Seen Only)",
x = "Coverage (Top Confidence)",
y = "Accuracy",
color = "Model"
) +
theme_minimal(base_size = 13)
ggsave("rejection_curve.pdf", width = 8, height = 5)
10**2.6
print(confusionMatrix(tan_pred, test$label))
library(readr)
library(dplyr)
library(purrr)
library(ggplot2)
library(glmnet)
library(caret)
library(bnlearn)
library(tidyr)
library(RColorBrewer)
library(cowplot)
library(scales)
time_start <- Sys.time()
train <- read_delim("zip.train", delim = " ", col_names = FALSE)
test <- read_delim("zip.test", delim = " ", col_names = FALSE)
new_colnames <- c("label", paste0("X", rep(1:16, each = 16), "x", rep(1:16, times = 16)))
n_levels <- 3
train <- train %>%
select(!X258) %>%
rename_with(~ new_colnames, everything()) %>%
mutate(across("label", as.factor)) %>%
mutate(across(-label, ~ cut(.x, breaks = seq(-1, 1, 2 / n_levels), ordered_result = TRUE, include.lowest = TRUE))) %>%
as.data.frame()
test <- test %>%
rename_with(~ new_colnames, everything()) %>%
mutate(across("label", as.factor)) %>%
mutate(across(-label, ~ cut(.x, breaks = seq(-1, 1, 2 / n_levels), ordered_result = TRUE, include.lowest = TRUE))) %>%
as.data.frame()
cat("Data loaded and preprocessed:\n")
print(Sys.time() - time_start)
excluded_digit <- "5"
train_ood <- train %>% filter(label != excluded_digit)
train_ood$label <- droplevels(train_ood$label)
compute_entropy <- function(probs) {
-sum(probs * log(probs + 1e-10))
}
results_list <- list()
# Naive Bayes
nb_structure <- naive.bayes(train_ood, "label")
nb_model <- bn.fit(nb_structure, train_ood, method = "bayes")
nb_preds <- predict(nb_model, test, prob = TRUE)
nb_probs <- attr(nb_preds, "prob")
results_list[["nb"]] <- data.frame(
model = "nb",
true_label = test$label,
predicted_label = nb_preds,
max_prob = apply(nb_probs, 2, max),
entropy = apply(nb_probs, 2, compute_entropy),
is_ood = test$label == excluded_digit
)
cat("Naive Bayes done\n")
# TAN
tan_structure <- tree.bayes(train_ood, "label")
tan_model <- bn.fit(tan_structure, train_ood, method = "bayes")
tan_preds <- predict(tan_model, test, prob = TRUE)
tan_probs <- attr(tan_preds, "prob")
results_list[["tan"]] <- data.frame(
model = "tan",
true_label = test$label,
predicted_label = tan_preds,
max_prob = apply(tan_probs, 2, max),
entropy = apply(tan_probs, 2, compute_entropy),
is_ood = test$label == excluded_digit
)
cat("TAN done\n")
print(confusionMatrix(tan_pred, test$label))
print(confusionMatrix(tan_preds, test$label))
print(confusionMatrix(logit_preds, test$label))
logit_preds
print(confusionMatrix(factor(logit_preds, levels = 0:9), test$label))
